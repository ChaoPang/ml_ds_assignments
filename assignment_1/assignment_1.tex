\documentclass[11pt]{report}
 
\usepackage{amssymb}
\usepackage{amsmath}

\begin{document}

\title{COMS 4721: Machine Learning for Data Science 
Home work 1}
\author{Chao Pang}
 
\maketitle

\section{Problem 1}
Imagine you have a sequence of N observations $(x_1,...,x_N)$, where each $x_i \in \{0,1,2,...,\infty\}$. You
model this sequence as i.i.d. from a Poisson distribution with unknown parameter $\lambda \in \mathbb{R}_+$, where

\[ p(X|\lambda) = \frac{ \lambda^X}{X!}e^-\lambda \]

\begin{flushleft}
(a) Solution: 
\end{flushleft}

\begin{center}
Let $L(\lambda; x_1, x_2 ..., x_i)  \sim$ joint likelihood
\end{center}

\begin{equation*}
\begin{split}
L(\lambda; x_1, x_2 ..., x_i)  & = \prod_{i=1}^N p(X=x_i| \lambda) \\
&  = \prod_{i=1}^N\frac{\lambda^{x_i}}{x_i!} e^{-\lambda} \\
\end{split}
\end{equation*}


\begin{flushleft}
(b) Solution: 
Use the logarithmic trick on the likelihood function
\end{flushleft}

\begin{equation*}
\begin{split}
\lambda_{ML} & = \underset{\lambda}{\mathrm{argmax}} \; \ln{(\prod_{i=1}^N\frac{\lambda^{x_i}}{x_i!} e^{-\lambda})} \\
& = \underset{\lambda}{\mathrm{argmax}} \; {\sum_{i=1}^N{x_i}\ln{\lambda} }  - {N\lambda}  -  {\sum_{i=1}^N\ln{x_i!}}\\
\end{split}
\end{equation*}

\begin{flushleft}
Take the derivative w.r.t $\boldsymbol{\lambda}$, set the gradient to 0, then solve $\boldsymbol{\lambda}$. The solution is equal to the sample mean. 
\end{flushleft}

\begin{equation*}
\nabla{L_{\lambda_{ML}}} = {\frac{\sum_{i=1}^N{x_i}}{\lambda}} - N = 0
\end{equation*}

\begin{equation*}
\lambda_{ML} = {\frac{\sum_{i=1}^N{x_i}}{N}}
\end{equation*}
\\
\begin{flushleft}
(c) Solution: 
\end{flushleft}

\begin{align*} 
& \textit{Bayes Rule:} \;\;  p(\lambda|X) = \frac{p(X|\lambda) p(\lambda|a,b)}{\int_{0}^{\infty} p(X|\lambda) p(\lambda|a,b) d\lambda} = \frac{p(X|\lambda) p(\lambda|a,b)}{p(X)} \\
&\textit{Let} \;\; p(\lambda|a, b)  = \frac{b^a\lambda^{a-1}e^{-b\lambda}}{\Gamma(a)} \textit{ for a, b $>$ 0 $\lambda$ $>$ 0} \\
&\textit{Let} \;\; p(X|\lambda) = \prod_{i=1}^N\frac{\lambda^{x_i}}{x_i!} e^{-\lambda} \\
\end{align*}

\begin{flushleft}
Then we have the following, again using the logarithmic trick
\end{flushleft}

\begin{equation*}
\begin{split}
\lambda_{MAP} & = \underset{\lambda}{\mathrm{argmax}} \; \ln{(\prod_{i=1}^N\frac{\lambda^{x_i}}{x_i!} e^{-\lambda} \frac{b^a\lambda^{a-1}e^{-b\lambda}}{\Gamma(a)})} \\
& = \underset{\lambda}{\mathrm{argmax}} \; {\sum_{i=1}^N{x_i}\ln{\lambda} }  - {N\lambda}  -  {\sum_{i=1}^N\ln{x_i!}} + a\ln{b} + (a-1)\ln{\lambda} - b\lambda - \ln{\Gamma(a)}\\
\end{split}
\end{equation*}

\begin{flushleft}
Next, take the derivative w.r.t $\boldsymbol{\lambda}$, set the gradient to 0, then solve $\boldsymbol{\lambda}$.
\end{flushleft}

\begin{equation*}
\nabla{L_{\lambda_{MAP}}} = {\frac{\sum_{i=1}^N{x_i}}{\lambda}} - N + \frac{a-1}{\lambda} - b = 0
\end{equation*}

\begin{equation*}
\lambda_{MAP} = \frac{{\sum_{i=1}^N{x_i}} + a - 1}{N + b}
\end{equation*}

 \pagebreak

\begin{flushleft}
(d) Solution: the prior gamma distribution is conjugate to the likelihood function, therefore the posterior is a gamma distribution as well. 
\end{flushleft}
\begin{equation*}
\begin{split}
p(\lambda|X) &\propto p(X|\lambda) p(\lambda|a,b) \\
&\propto (\prod_{i=1}^N\frac{\lambda^{x_i}}{x_i!} e^{-\lambda} \frac{b^a\lambda^{a-1}e^{-b\lambda}}{\Gamma(a)}) \\
&\propto [\lambda^{\sum_{i=1}^Nx_i} e^{-N\lambda}][\lambda^{a-1}e^{-b\lambda}] \\
&\propto \lambda^{\sum_{i=1}^Nx_i + a - 1} e^{-\lambda(b + N)} \\
&\propto \frac{b^{\sum_{i=1}^Nx_i + a}\lambda^{\sum_{i=1}^Nx_i + a - 1} e^{-\lambda(b + N)}}{\Gamma(\sum_{i=1}^Nx_i + a)} \\
& = \textit{Gamma}(\sum_{i=1}^Nx_i + a, b + N)
\end{split}
\end{equation*}

\begin{flushleft}
(e) Solution
\end{flushleft}
\begin{align*}
&\textit{Let $p(\lambda|X) \sim Gamma(\hat{a}, \hat{b})$ \; for $\hat{a} = \sum_{i=1}^Nx_i + a$ , $\hat{b} = b+ N $} \\
&\textit{We have $\sim Var[\lambda|X] = E[\lambda^2] - E[\lambda]^2$}
\end{align*}

\begin{equation*}
\begin{split}
E[\lambda|X] &= \int^{\infty}_0\frac{ \hat{b}^{\hat{a} } \lambda^{\hat{a} - 1} e^{-\lambda\hat{b}}}{\Gamma(\hat{a})} \lambda d\lambda \\
&= \frac{\hat{a}}{\hat{b}} \int^{\infty}_0 \frac{\hat{b}^{\hat{a} + 1} \lambda^{\hat{a}} e^{-\lambda \hat{b}} }{\Gamma{(\hat{a} + 1)}} d\lambda \\
&= \frac{\hat{a}}{\hat{b}} \int^{\infty}_0 Gamma(\lambda|\hat{a} + 1, \hat{b}) d\lambda \\
&= \frac{\hat{a}}{\hat{b}} = \frac{{\sum_{i=1}^N{x_i}} + a}{N + b} \\
\end{split} 
\end{equation*}

\begin{equation*}
\begin{split}
E[\lambda^2|X] &= \int^{\infty}_0\frac{ \hat{b}^{\hat{a} } \lambda^{\hat{a} - 1} e^{-\lambda\hat{b}}}{\Gamma(\hat{a})} \lambda^2 d\lambda \\
&= \frac{\hat{a} (\hat{a} + 1)}{\hat{b}^2} \int^{\infty}_0 \frac{\hat{b}^{\hat{a} + 2} \lambda^{\hat{a} + 1} e^{-\lambda \hat{b}} }{\Gamma{(\hat{a} + 2)}} d\lambda \\
&= \frac{\hat{a} (\hat{a} + 1)}{\hat{b}^2} \int^{\infty}_0 Gamma(\lambda|\hat{a} + 2, \hat{b}) d\lambda \\
&= \frac{\hat{a} (\hat{a} + 1)}{\hat{b}^2} \\
Var[\lambda|X] &= \frac{\hat{a} (\hat{a} + 1)}{\hat{b}^2}  - (\frac{\hat{a}}{\hat{b}})^2 = \frac{\hat{a}}{\hat{b}^2}
\end{split} 
\end{equation*}
\\
The relations between $\lambda_{ML}$, $\lambda_{MAP}$, and $E[\lambda|X]$ is described below. 
\begin{itemize}
\item $\lambda_{MAP}$ and $E[\lambda|X]$ are almost identical to each other. $\lambda_{MAP}$ is $\frac{1}{N}$ smaller than $E[\lambda|X]$. As the number of observation N increases, $\lambda_{MAP}$ approaches $E[\lambda|X]$
\item in the limit as $a \xrightarrow{} 0$ and $b \xrightarrow{} 0$, $E[\lambda|X]$ approaches $\lambda_{ML}$
\end{itemize}

\section{Problem 2}
\subsection {}
(a) You have data $(x_i; y_i)$ for i = 1,..., n, where x $\in  \mathbb{R}$ and y $\in \mathbb{R}$. You model this as $y_i \stackrel{iid}{\sim} N(x_i^Tw; \sigma^2)$. You use the data you have to approximate w with $w_{RR}= (\lambda I + X^TX)^{-1}X^T y$ , where X
and y are defined as in the lectures. Derive the results for $\mathbb{E}[w_{RR}]$ and $\mathbb{V}[w_{RR}]$ given in the slides.

\begin{flushleft}
Solution: the subscript RR is dropped from the parameter $w_{RR}$ to keep the notation uncluttered
\end{flushleft}
\begin{equation*}
\begin{split}
E[w] &=  E[(\lambda I + X^TX)^{-1}X^T y] \\
&= (\lambda I + X^TX)^{-1}X^T E[y] \\
&= (\lambda I + X^TX)^{-1}X^TXw \\
\\
Var[w] &=  E[(w - E[w])(w - E[w])^T] \\
&=  E[w w^T] - E[w]E[w]^T \\
\end{split} 
\end{equation*}

\begin{flushleft}
Plug in $w_{RR}$ into $Var[w]$
\end{flushleft}

\begin{equation*}
\begin{split}
Var[w] =  &E[(\lambda I + X^TX)^{-1}X^Ty  y^T X (\lambda I + X^TX)^{-1}] \\
& - (\lambda I + X^TX)^{-1}X^TXw w^T X^T X (\lambda I + X^TX)^{-1}\\
= &(\lambda I + X^TX)^{-1}X^T E[y y^T] X (\lambda I + X^TX)^{-1}] \\
& - (\lambda I + X^TX)^{-1}X^TXw w^T X^T X (\lambda I + X^TX)^{-1}\\
\end{split} 
\end{equation*}

\begin{flushleft}
Plug in $E[yy^T] = \sigma^2I + Xww^TX^T$
\end{flushleft}

\begin{equation*}
\begin{split}
Var[w] = &(\lambda I + X^TX)^{-1}X^T (\sigma^2I + Xww^TX^T) X (\lambda I + X^TX)^{-1} \\
& - (\lambda I + X^TX)^{-1}X^TXw w^T X^T X (\lambda I + X^TX)^{-1}\\
= &(\lambda I + X^TX)^{-1}X^T \sigma^2I X (\lambda I + X^TX)^{-1}  \\
= &\sigma^2 (\lambda I + X^TX)^{-1}X^T X (\lambda I + X^TX)^{-1}  \\
= &\sigma^2 (\lambda X^TX(X^TX)^{-1} + X^TX)^{-1}X^T X(\lambda X^TX(X^TX)^{-1} + X^TX)^{-1}  \\
= &\sigma^2 (X^TX(\lambda (X^TX)^{-1} + I))^{-1}X^T X (X^TX(\lambda (X^TX)^{-1} + I))^{-1}  \\
= &\sigma^2 (\lambda (X^TX)^{-1} + I)^{-1} (X^TX)^{-1} X^T X (\lambda (X^TX)^{-1} + I)^{-1} (X^TX)^{-1} \\
= &\sigma^2 (\lambda (X^TX)^{-1} + I)^{-1} (\lambda (X^TX)^{-1} + I)^{-1} (X^TX)^{-1} \\
= &\sigma^2 (\lambda (X^TX)^{-1} + I)^{-1} ((\lambda (X^TX)^{-1} + I)^{-1} (X^TX)^{-1})^T \\
= &\sigma^2 (I + \lambda(X^TX)^{-1})^{-1}  (X^TX)^{-1} ((I + \lambda(X^TX)^{-1})^{-1})^T \\
= &\sigma^2 Z (X^TX)^{-1} Z^T \\
\text{where } Z = &(I + \lambda (X^TX)^{-1})^{-1}
\end{split} 
\end{equation*}

\pagebreak

\subsection {}
(b) If $w_{RR}$ is the ridge regression solution and $w_{LS}$ is the least squares solution for the above problem,derive an equation for writing $w_{RR}$ as a function of $w_{LS}$ and the singular values and right singular vectorsof feature matrix X. Recall that the singular value decomposition of $X = USV^T$.

\begin{flushleft}
Solution: From the lecture, we have the following
\end{flushleft}
\begin{align*}
&w_{LS} = (X^T X)^{-1} X^T y \\
&w_{RR} = (I \lambda + X^T X)^{-1} X^T y \\
&X = USV^T
\end{align*}

\begin{flushleft}
Then we can derive
\end{flushleft}
\begin{equation*}
\begin{split}
w_{RR} &= ((X^TX) (\lambda (X^TX)^{-1} + I))^{-1} X^T y \\
&= (I + \lambda (X^TX)^{-1})^{-1} (X^TX)^{-1} X^T y \\
&= (I + \lambda (X^TX)^{-1})^{-1} w_{LS} \\
&= (I + \lambda (V S^T U^T U S V^T)^{-1})^{-1} w_{LS} \\
&= (I + \lambda V S^{-2} V^T)^{-1} w_{LS} \\
\end{split} 
\end{equation*}


\end{document}