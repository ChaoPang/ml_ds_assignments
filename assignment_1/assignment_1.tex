\documentclass[11pt]{report}
 
\usepackage{ragged2e}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{graphicx}

\graphicspath{ {hw1-data/} }

\begin{document}

\title{COMS 4721: Machine Learning for Data Science 
Home work 1}
\author{Chao Pang}
 
\maketitle

\section{Problem 1}
Imagine you have a sequence of N observations $(x_1,...,x_N)$, where each $x_i \in \{0,1,2,...,\infty\}$. You
model this sequence as i.i.d. from a Poisson distribution with unknown parameter $\lambda \in \mathbb{R}_+$, where

\[ p(X|\lambda) = \frac{ \lambda^X}{X!}e^-\lambda \]

\justify
(a) Solution: 

\begin{center}
Let $L(\lambda; x_1, x_2 ..., x_i)  \sim$ joint likelihood
\end{center}

\begin{equation*}
\begin{split}
L(\lambda; x_1, x_2 ..., x_i)  & = \prod_{i=1}^N p(X=x_i| \lambda) \\
&  = \prod_{i=1}^N\frac{\lambda^{x_i}}{x_i!} e^{-\lambda} \\
\end{split}
\end{equation*}


\justify
(b) Solution: 
Use the logarithmic trick on the likelihood function

\begin{equation*}
\begin{split}
\lambda_{ML} & = \underset{\lambda}{\mathrm{argmax}} \; \ln{(\prod_{i=1}^N\frac{\lambda^{x_i}}{x_i!} e^{-\lambda})} \\
& = \underset{\lambda}{\mathrm{argmax}} \; {\sum_{i=1}^N{x_i}\ln{\lambda} }  - {N\lambda}  -  {\sum_{i=1}^N\ln{x_i!}}\\
\end{split}
\end{equation*}

\justify
Take the derivative w.r.t $\boldsymbol{\lambda}$, set the gradient to 0, then solve for $\boldsymbol{\lambda}$. The solution is equal to the sample mean. 

\begin{equation*}
\nabla{L_{\lambda_{ML}}} = {\frac{\sum_{i=1}^N{x_i}}{\lambda}} - N = 0
\end{equation*}

\begin{equation*}
\lambda_{ML} = {\frac{\sum_{i=1}^N{x_i}}{N}}
\end{equation*}
\\

\justify
(c) Solution: 

\begin{align*} 
& \textit{Bayes Rule:} \;\;  p(\lambda|X) = \frac{p(X|\lambda) p(\lambda|a,b)}{\int_{0}^{\infty} p(X|\lambda) p(\lambda|a,b) d\lambda} = \frac{p(X|\lambda) p(\lambda|a,b)}{p(X)} \\
&\textit{Let} \;\; p(\lambda|a, b)  = \frac{b^a\lambda^{a-1}e^{-b\lambda}}{\Gamma(a)} \textit{ for a, b $>$ 0 $\lambda$ $>$ 0} \\
&\textit{Let} \;\; p(X|\lambda) = \prod_{i=1}^N\frac{\lambda^{x_i}}{x_i!} e^{-\lambda} \\
\end{align*}

\justify
Then we have the following, again using the logarithmic trick

\begin{equation*}
\begin{split}
\lambda_{MAP} & = \underset{\lambda}{\mathrm{argmax}} \; \ln{(\prod_{i=1}^N\frac{\lambda^{x_i}}{x_i!} e^{-\lambda} \frac{b^a\lambda^{a-1}e^{-b\lambda}}{\Gamma(a)})} \\
& = \underset{\lambda}{\mathrm{argmax}} \; {\sum_{i=1}^N{x_i}\ln{\lambda} }  - {N\lambda}  -  {\sum_{i=1}^N\ln{x_i!}} + a\ln{b} + (a-1)\ln{\lambda} - b\lambda - \ln{\Gamma(a)}\\
\end{split}
\end{equation*}

\justify
Next, take the derivative w.r.t $\boldsymbol{\lambda}$, set the gradient to 0, then solve for $\boldsymbol{\lambda}$.

\begin{equation*}
\nabla{L_{\lambda_{MAP}}} = {\frac{\sum_{i=1}^N{x_i}}{\lambda}} - N + \frac{a-1}{\lambda} - b = 0
\end{equation*}

\begin{equation*}
\lambda_{MAP} = \frac{{\sum_{i=1}^N{x_i}} + a - 1}{N + b}
\end{equation*}

 \pagebreak

\justify
(d) Solution: the prior gamma distribution is conjugate to the likelihood function, therefore the posterior is a gamma distribution as well. 

\begin{equation*}
\begin{split}
p(\lambda|X) &\propto p(X|\lambda) p(\lambda|a,b) \\
&\propto (\prod_{i=1}^N\frac{\lambda^{x_i}}{x_i!} e^{-\lambda} \frac{b^a\lambda^{a-1}e^{-b\lambda}}{\Gamma(a)}) \\
&\propto [\lambda^{\sum_{i=1}^Nx_i} e^{-N\lambda}][\lambda^{a-1}e^{-b\lambda}] \\
&\propto \lambda^{\sum_{i=1}^Nx_i + a - 1} e^{-\lambda(b + N)} \\
&\propto \frac{b^{\sum_{i=1}^Nx_i + a}\lambda^{\sum_{i=1}^Nx_i + a - 1} e^{-\lambda(b + N)}}{\Gamma(\sum_{i=1}^Nx_i + a)} \\
& = \textit{Gamma}(\sum_{i=1}^Nx_i + a, b + N)
\end{split}
\end{equation*}

\justify
(e) Solution

\begin{align*}
&\textit{Let $p(\lambda|X) \sim Gamma(\hat{a}, \hat{b})$ \; for $\hat{a} = \sum_{i=1}^Nx_i + a$ , $\hat{b} = b+ N $} \\
&\textit{We have $\sim Var[\lambda|X] = E[\lambda^2] - E[\lambda]^2$}
\end{align*}

\begin{equation*}
\begin{split}
E[\lambda|X] &= \int^{\infty}_0\frac{ \hat{b}^{\hat{a} } \lambda^{\hat{a} - 1} e^{-\lambda\hat{b}}}{\Gamma(\hat{a})} \lambda d\lambda \\
&= \frac{\hat{a}}{\hat{b}} \int^{\infty}_0 \frac{\hat{b}^{\hat{a} + 1} \lambda^{\hat{a}} e^{-\lambda \hat{b}} }{\Gamma{(\hat{a} + 1)}} d\lambda \\
&= \frac{\hat{a}}{\hat{b}} \int^{\infty}_0 Gamma(\lambda|\hat{a} + 1, \hat{b}) d\lambda \\
&= \frac{\hat{a}}{\hat{b}} = \frac{{\sum_{i=1}^N{x_i}} + a}{N + b} \\
\end{split} 
\end{equation*}

\begin{equation*}
\begin{split}
E[\lambda^2|X] &= \int^{\infty}_0\frac{ \hat{b}^{\hat{a} } \lambda^{\hat{a} - 1} e^{-\lambda\hat{b}}}{\Gamma(\hat{a})} \lambda^2 d\lambda \\
&= \frac{\hat{a} (\hat{a} + 1)}{\hat{b}^2} \int^{\infty}_0 \frac{\hat{b}^{\hat{a} + 2} \lambda^{\hat{a} + 1} e^{-\lambda \hat{b}} }{\Gamma{(\hat{a} + 2)}} d\lambda \\
&= \frac{\hat{a} (\hat{a} + 1)}{\hat{b}^2} \int^{\infty}_0 Gamma(\lambda|\hat{a} + 2, \hat{b}) d\lambda \\
&= \frac{\hat{a} (\hat{a} + 1)}{\hat{b}^2} \\
Var[\lambda|X] &= \frac{\hat{a} (\hat{a} + 1)}{\hat{b}^2}  - (\frac{\hat{a}}{\hat{b}})^2 = \frac{\hat{a}}{\hat{b}^2}
\end{split} 
\end{equation*}
\\
The relations between $\lambda_{ML}$, $\lambda_{MAP}$, and $E[\lambda|X]$ is described below. 
\begin{itemize}
\item $\lambda_{MAP}$ and $E[\lambda|X]$ are almost identical to each other. $\lambda_{MAP}$ is $\frac{1}{N}$ smaller than $E[\lambda|X]$. As the number of observation N increases, $\lambda_{MAP}$ approaches $E[\lambda|X]$
\item in the limit as $a \xrightarrow{} 0$ and $b \xrightarrow{} 0$, $E[\lambda|X]$ approaches $\lambda_{ML}$
\end{itemize}

\section{Problem 2}
\justify
(a) You have data $(x_i; y_i)$ for i = 1,..., n, where x $\in  \mathbb{R}$ and y $\in \mathbb{R}$. You model this as $y_i \stackrel{iid}{\sim} N(x_i^Tw; \sigma^2)$. You use the data you have to approximate w with $w_{RR}= (\lambda I + X^TX)^{-1}X^T y$ , where X
and y are defined as in the lectures. Derive the results for $\mathbb{E}[w_{RR}]$ and $\mathbb{V}[w_{RR}]$ given in the slides.

\justify
\textbf{Solution:}

\begin{equation*}
\begin{split}
E[w_{RR}] &=  E[(\lambda I + X^TX)^{-1}X^T y] \\
&= (\lambda I + X^TX)^{-1}X^T E[y] \\
&= (\lambda I + X^TX)^{-1}X^TXw \\
\\
Var[w] &=  E[(w - E[w])(w - E[w])^T] \\
&=  E[w w^T] - E[w]E[w]^T \\
\end{split} 
\end{equation*}

\justify
Plug in $w_{RR}$ into $Var[w]$

\begin{equation*}
\begin{split}
Var[w_{RR}] =  & E[(\lambda I + X^TX)^{-1}X^Ty  y^T X (\lambda I + X^TX)^{-1}] \\
& - (\lambda I + X^TX)^{-1}X^TXw w^T X^T X (\lambda I + X^TX)^{-1}\\
= &(\lambda I + X^TX)^{-1}X^T E[y y^T] X (\lambda I + X^TX)^{-1}] \\
& - (\lambda I + X^TX)^{-1}X^TXw w^T X^T X (\lambda I + X^TX)^{-1}\\
\end{split} 
\end{equation*}

\justify
$E[yy^T]$ can be derived as the following, where $y \sim N(\mu, \sum)$ for $\mu = Xw$

\begin{equation*}
\begin{split}
Var[y] =&  E(y - E[y]) E(y - E[y])^T \\
=& E[y y^T] - E[\mu \mu^T] = \sum \\
E[yy^T] =& \sigma^2I + Xww^TX^T\\
\end{split} 
\end{equation*}


\begin{equation*}
\begin{split}
Var[w_{RR}] = &(\lambda I + X^TX)^{-1}X^T (\sigma^2I + Xww^TX^T) X (\lambda I + X^TX)^{-1} \\
& - (\lambda I + X^TX)^{-1}X^TXw w^T X^T X (\lambda I + X^TX)^{-1}\\
= &(\lambda I + X^TX)^{-1}X^T \sigma^2I X (\lambda I + X^TX)^{-1}  \\
= &\sigma^2 (\lambda I + X^TX)^{-1}X^T X (\lambda I + X^TX)^{-1}  \\
= &\sigma^2 (\lambda X^TX(X^TX)^{-1} + X^TX)^{-1}X^T X(\lambda X^TX(X^TX)^{-1} + X^TX)^{-1}  \\
= &\sigma^2 (X^TX(\lambda (X^TX)^{-1} + I))^{-1}X^T X (X^TX(\lambda (X^TX)^{-1} + I))^{-1}  \\
= &\sigma^2 (\lambda (X^TX)^{-1} + I)^{-1} (X^TX)^{-1} X^T X (\lambda (X^TX)^{-1} + I)^{-1} (X^TX)^{-1} \\
= &\sigma^2 (\lambda (X^TX)^{-1} + I)^{-1} (\lambda (X^TX)^{-1} + I)^{-1} (X^TX)^{-1} \\
\\
\end{split} 
\end{equation*}

\justify
Given that  $(X^TX)^{-1}$ and $\lambda (X^TX)^{-1} + I)^{-1} (X^TX)^{-1}$ are symmetric matrices so we can replace them with their corresponding transpose. 

\begin{equation*}
\begin{split}
Var[w_{RR}] = &\sigma^2 (\lambda (X^TX)^{-1} + I)^{-1} ((\lambda (X^TX)^{-1} + I)^{-1} (X^TX)^{-1})^T \\
= &\sigma^2 (I + \lambda(X^TX)^{-1})^{-1}  (X^TX)^{-1} ((I + \lambda(X^TX)^{-1})^{-1})^T \\
= &\sigma^2 Z (X^TX)^{-1} Z^T \\
\\
\text{where } Z = &(I + \lambda (X^TX)^{-1})^{-1}
\end{split} 
\end{equation*}

\pagebreak

\justify
(b) If $w_{RR}$ is the ridge regression solution and $w_{LS}$ is the least squares solution for the above problem,
derive an equation for writing $w_{RR}$ as a function of $w_{LS}$ and the singular values and right singular vectors
of feature matrix X. Recall that the singular value decomposition of $X = USV^T$.

\justify
\textbf{Solution:} From the lecture, we have the following

\begin{align*}
&w_{LS} = (X^T X)^{-1} X^T y \\
&w_{RR} = (I \lambda + X^T X)^{-1} X^T y \\
&X = USV^T
\end{align*}

\justify
Then we can derive

\begin{equation*}
\begin{split}
w_{RR} &= ((X^TX) (\lambda (X^TX)^{-1} + I))^{-1} X^T y \\
&= (I + \lambda (X^TX)^{-1})^{-1} (X^TX)^{-1} X^T y \\
&= (I + \lambda (X^TX)^{-1})^{-1} w_{LS} \\
&= (I + \lambda (V S^T U^T U S V^T)^{-1})^{-1} w_{LS} \\
&= (I + \lambda V S^{-2} V^T)^{-1} w_{LS} \\
&= (V I V^T + V \lambda S^{-2} V^T)^{-1} w_{LS} \\
&= (V (I + \lambda S^{-2}) V^T )^{-1} w_{LS} \\
&= (V^T)^{-1} (I + \lambda S^{-2})^{-1} V^{-1}  w_{LS} \\
&= V (I + \lambda S^{-2})^{-1} V^T  w_{LS} \\
\end{split} 
\end{equation*}

\pagebreak

\section{Problem 3}
(a) For $\lambda = 0, 1, 2 ,3, ..., 5000$, solve for $w_{RR}$. (Notice that when $\lambda = 0$, $w_{RR} = w_{LS}$). In one figure, plot the 7 values in $w_{RR}$ as a function of $df(\lambda)$. You will need to call a built in SVD function to do this as discussed in the slides. Be sure to label your 7 curves by their dimension in ${x^2}$.

\justify
\textbf{Answer:} \textbf{Figure~\ref{fig:df_lambda_weight}}  shows the plot of the ridge regression $w_{RR}$ solution as a function of degree of freedom for lambda. The weights are named based on labels provided in the footnoot of the homework, \emph{\textbf{bias term}} corresponds to the column, where all the values are 1.0. 

\begin{figure}[h]
\includegraphics[scale=0.40]{df_lambda_weight.png}
\centering
\caption{Plot weight parameters against the corresponding degree of freedom for lambda}
\label{fig:df_lambda_weight}
\end{figure}

\justify
(b) Two dimensions clearly stand out over the others. Which ones are they and what information can we get from this?

\justify
\textbf{Answer:} as \textbf{Figure~\ref{fig:df_lambda_weight}} shows that \emph{\textbf{weight}} (corresponding to the bottom line) and \emph{\textbf{year made}} (corresponding to the top line) are the dimensions that stand out over the others. \emph{\textbf{weight}} is negatively correlated with the response (miles per gallon) meaning the increase in corresponding feature will decrease the value of the response. On the other hand, \emph{\textbf{year made}} is positively correlated with the response meaning the increase in corresponding feature will increase the value of the response. Regardless of the choice of $\lambda$,  \emph{\textbf{year made}} always has a positive relationship with the output and \emph{\textbf{weight}} always has a negative relationship with the output. In addition, \emph{\textbf{year made}} and \emph{\textbf{weight}} always have the largest values among all the other dimensions, therefore we can be confident that the features corresponding to \emph{\textbf{year made}} and \emph{\textbf{weight}} are most significant. 

\justify
(c) For $\lambda = 1, ..., 50$, predict all 42 test cases. Plot the root mean squared error (RMSE) on the test set as a function of $\lambda$. What does this figure tell you when choosing for this problem (and when choosing between ridge regression and least squares)?

\justify
\textbf{Answer:} as \textbf{Figure~\ref{fig:ambda_rmse}} shows that RMSE keeps increasing as $\lambda$ increases. The best solution based on the cross validation in the test set would be the least squares solution where $\lambda = 0$. This may suggest a high bias (underfitting) in the model using the default features. 


\begin{figure}[h]
\includegraphics[scale=0.35]{lambda_rmse.png}
\centering
\caption{Plot root mean squared error against the lambda}
\label{fig:ambda_rmse}
\end{figure}


\justify
(d) In one figure, plot the test RMSE as a function of $\lambda = 0, . . . , 100$ for $ p = 1,  2, 3$. Based on this plot, which value of p should you choose and why? How does your assessment of the ideal value of $\lambda$ change for this problem?

\justify
\textbf{Answer:} All features except for the bias term were standardized using the training data's mean and standard deviation for both training and test sets. \textbf{Figure~\ref{fig:polynomial_lambda_rmse}} shows that the \textbf{3rd-order} polynomial regression model performs better consistently than the other two models and it has the lowest RMSE value using the test set for $\lambda = 52$, therefore we should choose $p=3$.  

\begin{figure}[h]
\includegraphics[scale=0.4]{polynomial_lambda_rmse.png}
\centering
\caption{Plot root mean squared error against the lambda for pth-order polynomial regressions}
\label{fig:polynomial_lambda_rmse}
\end{figure}

\justify
The best performing $\lambda$ values that minimize RMSE for different models can be shown below:
\begin{itemize}
	\item 1st order polynomial regression: $\lambda = 0, RMSE_{min} \approx 2.634$
	\item 2nd order polynomial regression: $\lambda = 51, RMSE_{min} \approx 2.13$
	\item 3rd order polynomial regression: $\lambda = 52, RMSE_{min} \approx 2.10$
\end{itemize} 

\end{document}