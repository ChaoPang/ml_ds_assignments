\documentclass[11pt]{report}
 
\usepackage{ragged2e}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{bbm}
\usepackage{tablefootnote}
\usepackage{longtable}
\usepackage{csvsimple}
\usepackage{booktabs}

\graphicspath{ {hw1-data/} }

\begin{document}

\title{COMS 4721: Machine Learning for Data Science 
Home work 2}
\author{Chao Pang}
 
\maketitle

\section* {Problem 1}
Derive the solution for $\pi$ and each $\lambda_{y,d}$ by maximizing 

\[ \hat{\pi}, \hat{\lambda}_{0, 1:D}, \hat{\lambda}_{1, 1:D}  =  arg \underset{\hat{\pi}, \hat{\lambda}_{0, 1:D}, \hat{\lambda}_{1, 1:D} }{max} \sum_{i=1}^{n} \ln {p(y_{i}|\pi)} +  \sum_{d=1}^{D} \Biggl( \ln{p(\lambda_{0,d})} + \ln{p(\lambda_{1,d})}  + \sum_{i=1}^{n} \ln{p(x_{i,d} | \lambda_{y_{i},d}) \Biggr)} \]


\begin{align*} 
& \textit{Let} \;\; p(y | \pi)  = \pi^{y} (1 - \pi)^{1 - y}   \;\; \textit{where y $\in \{0, 1\}$}\\
& \textit{Let} \;\; p(\lambda | y)  = Gamma(2, 1) = \frac{ \lambda e^{-\lambda} } {\Gamma(2)}  \\
& \textit{Let} \;\; p(x | \lambda) = \frac{ \lambda^x} {x!}e^{-\lambda} \\
\end{align*}

\justify (a) Derive $\hat{\pi}$ using the objective above
\justify Solution: first plug in $p(y | \pi) $ and then take the derivative w.r.t \textbf{w}. 

\begin{align*} 
L &= \sum_{i=1}^{n} \Biggl( \ln{\pi^{ \mathbbm{1}\{ y_{i} = y \}  } (1 - \pi)^{ 1 - \mathbbm{1}\{ y_{i} = y \}  } } \Biggr) + \sum_{d=1}^{D} \Biggl( \ln{p(\lambda_{0,d})} + \ln{p(\lambda_{1,d})}  + \sum_{i=1}^{n} \ln{p(x_{i,d} | \lambda_{y_{i},d})} \Biggr) \\
L &= \sum_{i=1}^{n} \Biggl( \mathbbm{1}\{ y_{i} = y \}  \ln{\pi} + ({ 1 - \mathbbm{1}\{ y_{i} = y \}  })Â \ln{(1 - \pi)}  \Biggr) \\
& + \sum_{d=1}^{D} \Biggl( \ln{p(\lambda_{0,d})} + \ln{p(\lambda_{1,d})}  + \sum_{i=1}^{n} \ln{p(x_{i,d} | \lambda_{y_{i},d})} \Biggr) \\
\\
\nabla{L_{\pi}} &= \sum_{i=1}^{n} \Biggl( \frac{ \mathbbm{1}\{ y_{i} = y \}  }{\pi} - \frac{ 1 - \mathbbm{1}\{ y_{i} = y \}  } { 1 - \pi}  \Biggr)  = 0 \\
\\
\pi &= \frac { \sum_{i=1}^{n} \mathbbm{1}\{ y_{i} = y \} } {n} \\ 
\end{align*}

\pagebreak

\justify (b) Derive $\hat{\lambda}_{y, d}$ using the objective above, leaving $y$ and $d$ arbitrary in your notation
\justify Solution: first plug in $p(\lambda | y)$ and $p(x | \lambda)$, then take the derivative w.r.t $\lambda_{y,d}$. 

\[
L= \sum_{i=1}^{n} \ln {p(y_{i}|\pi)} +  \sum_{d=1}^{D} \Biggl( \ln{ \frac{ \lambda_{0,d} \; e^{-\lambda_{0,d}} } {\Gamma(2)} } + \ln{ \frac{ \lambda_{1,d} \; e^{-\lambda_{1,d}} } {\Gamma(2)} }  + \sum_{i=1}^{n} \ln{\frac{ \lambda_{y_{i}, d}^{x_{i,d}}} {x_{i,d}!}e^ {-\lambda_{y_{i},d}} \Biggr)}
\]

\-
\justify Ignore the first term because $\sum_{i=1}^{n} \ln {p(y_{i}|\pi)}$ doesn't depend on $\lambda$. We can ignore the sum over dimension $d$ and remove $d$ notation temporarily since all dimensions can be treated the same way, simplify and rewrite the equation as the following, 

\begin{small}
\[
\nabla{
\Biggl(
\ln{ \frac{ \lambda_{0} \; e^{-\lambda_{0}} } {\Gamma(2)} } + \ln{ \frac{ \lambda_{1} \; e^{-\lambda_{1}} } {\Gamma(2)} }  
+ \sum_{i=1}^{n} \ln{\frac{ \lambda_{0}^{x_{i}}} {x_{i}!}e^ {-\lambda_{0}}  \mathbbm{1}\{ y_{i} = 0 \} } 
+ \sum_{i=1}^{n} \ln{\frac{ \lambda_{1}^{x_{i}}} {x_{i}!}e^ {-\lambda_{1}}  \mathbbm{1}\{ y_{i} = 1} \}
\Biggr)
}
\]
\end{small}

\-
\justify Solving for $\lambda_{0, d}$
\[
\sum_{i=1}^{n} \frac{ x_{i} } {\lambda_{0}} \mathbbm{1}\{ y_{i} = 0 \}   -  \sum_{i=1}^{n} \mathbbm{1}\{ y_{i} = 0 \}  + \frac {1} {\lambda_{0}} - 1 = 0 
\]

\[
\lambda_{0} = \frac {\sum_{i=1}^{n} x_{i} \mathbbm{1}\{ y_{i} = 0 \} + 1} {\sum_{i=1}^{n} \mathbbm{1}\{ y_{i} = 0 \} + 1} 
\]

\- 
\justify $\lambda_{1,d}$ can be derived in the same way, so the the equation could be generalized using $y$. In addition, we put the dimension $d$ back to the equation, therefore we have the following,

\[
\lambda_{y, d} = \frac {\sum_{i=1}^{n} x_{i, d} \; \mathbbm{1}\{ y_{i} = y \} + 1} {\sum_{i=1}^{n} \mathbbm{1}\{ y_{i} = y \} + 1}  
\]
where $y \in \{0, 1\}$

\pagebreak

\section* {Problem 2}
\justify (a) the confusion matrix and the accuracy for naive Bayes classifier are shown below

\begin{center}
\begin{tabular}{ |c|c|c| } 
 \hline
  & Predicted 0  & Predicted 1 \\ 
 \hline
 Actual 0 & TN = 2295 & FP = 492 \\ 
  \hline
Actual 1  & FN = 99 &  TP = 1714 \\ 
 \hline
\end{tabular}
\end{center}

\justify where TN is true negative, FP is false positive, FN is false negative, and TP is true positive. The overall accuracy is \textbf{$87.15\%$}


\justify (b) \textbf{Figure~\ref{fig:lambda_stem}} shows that the $\lambda$ values in 16th dimension (\textbf{free}) and 52nd dimension (\textbf{!}) are much higher in spam than non-spam, indicating that the presence of these two words are highly suggestive of a spam email. 

\begin{figure}[h]
\includegraphics[scale=0.5]{hw2-data/Bayes_classifier/lambda_stem.png}
\centering
\caption{Plot stem plot for 54 dimensions for spam and non-spam}
\label{fig:lambda_stem}
\end{figure}

\pagebreak

\justify (c) plot the training function per each iteration for 10 runs in the same figure for logistic regression
\begin{figure}[h]
\includegraphics[scale=0.4]{hw2-data/Bayes_classifier/logistic_regression_learning_objective.png}
\centering
\caption{plot the training function each iteration for 10 runs for logistic regression}
\label{fig:logistic_regression_learning_objective}
\end{figure}


\justify (d) Derive the update for $w_{t+1}$ for Newton's method

\[
L(w) \approx L^{'} (w)   \equiv L(w_{t}) + (w - w_{t})^{T} \nabla{L(w_{t})} + \frac{1}{2} (w - w_{t})^{T} \nabla^{2}{L(w_{t})} (w - w_{t})
\]

\justify Solution: set $w_{t+1} = arg\;max_{w} L^{'} (w) $ and $\nabla^{2}{L(w_{t})}$ is symmetric

\begin{align*}
L^{'} (w) =& L(w_{t}) + w^{T} \nabla{L(w_{t})}  - w_{t}^{T} \nabla{L(w_{t})} \\
&+ \frac{1}{2} \Biggl( w^{T} \nabla^{2}{L(w_{t})} w  - w^{T}\nabla^{2}{L(w_{t})}  w_{t} - w^{T}_{t} \nabla^{2}{L(w_{t})} w + w^{T}_{t} \nabla^{2}{L(w_{t})}  w_{t}\Biggr) \\
=& L(w_{t}) + w^{T} \nabla{L(w_{t})}  - w_{t}^{T} \nabla{L(w_{t})} \\
&+ \frac{1}{2} \Biggl( w^{T} \nabla^{2}{L(w_{t})} w  - 2w^{T}\nabla^{2}{L(w_{t})}  w_{t} + w^{T}_{t} \nabla^{2}{L(w_{t})}  w_{t} \Biggr)
\end{align*}

\justify Solve for w
\[
\nabla{L^{'}(w)} =  \nabla{L(w_{t}})^{T} + w^{T} \nabla^{2}{L(w_{t})} - w_{t}^{T} \nabla^{2}{L(w_{t})} = 0
\]

\[
\nabla^{2}{L(w_{t})}  w =   \nabla^{2}{L(w_{t})} w_{t} - \nabla{L(w_{t}})  \\
\]

\[
w=  w_{t} - \nabla^{2}{L(w_{t})}^{-1} \; \nabla{L(w_{t}}) 
\]

\-

\justify we have the first order derivative w.r.t to w 

\-

\[
\nabla{L(w}) = \sum_{i=1}^{n} (1 - \sigma_{i}(y_{i} \cdot w)) y_{i} x_{i}
\]

\-

\justify this can be written in the matrix form

\-

\[
\nabla{L(w}) = X^{T}SY
\]

\-

\justify $S$ is a n $\times$ n diagonal matrix, where $S_{ii} = 1 - \sigma_{i} ({y_{i} \cdot w})$. We need to compute the second order derivative w.r.t to w, the Hessian matrix

\-

\[
\nabla{L(w}) = \sum_{i=1}^{n} (1 - \frac{e^{y_{i} x_{i}^{T} w}} {(1 + e^{y_{i} x_{i}^{T} w})} ) y_{i} x_{i}
\]

\[
\nabla{L(w}) = \sum_{i=1}^{n} ( 1 - \frac{1} {(1 + e^{-y_{i} x_{i}^{T} w})} ) y_{i} x_{i}
\]

\[
\nabla^{2}{L(w}) = \sum_{i=1}^{n} - \frac{ e^{-y_{i} x_{i}^{T} w } } {(1 + e^{-y_{i} x_{i}^{T} w})^{2}}  y_{i}^{2} x_{i} x_{i}^{T}
\]

\[
\nabla^{2}{L(w}) = \sum_{i=1}^{n} - \sigma_{i} ({y_{i} \cdot w})( 1 - \sigma_{i} ({y_{i} \cdot w})) x_{i} x_{i}^{T}
\]

\-

\justify $\nabla^{2}{L(w})$ can be written in the matrix quadratic form 

\-


\[
\nabla^{2}{L(w}) = - X^{T} M X
\]

\-

\justify $M$ is a n $\times$ n diagonal matrix, where $M_{ii} = \sigma_{i} ({y_{i} \cdot w})( 1 - \sigma_{i} ({y_{i} \cdot w}))$. Finally, we can plug the gradient and Hessian matrix back in

\-

\[
w =  w_{t} + (X^{T} M X)^{-1} (X^{T}SY)
\]

\-
\justify $S$ is a n $\times$ n diagonal matrix, where $S_{ii} = 1 - \sigma_{i} ({y_{i} \cdot w})$,  and $M$ is a n $\times$ n diagonal matrix, where $M_{ii} = \sigma_{i} ({y_{i} \cdot w})( 1 - \sigma_{i} ({y_{i} \cdot w}))$. In doing the 10-fold cross validation, some of the runs threw an error that the matrix $X^{T} M X$ was not invertible, an identity matrix was added to $X^{T} M X$ to get around this problem. Therefore we have the following

\-

\[
w =  w_{t} + (X^{T} M X + I)^{-1} (X^{T}SY)
\]

\-

\justify \textbf{Figure~\ref{fig:newton_method_learning_objective}} shows that the learning objective increased much faster in Newton's method than in the steepest ascent algorithm shown in \textbf{Figure~\ref{fig:logistic_regression_learning_objective}}. All the runs in Newton's method seemed to have converged after 10 iterations, whereas it took a few hundred iterations for the algorithm to converge in the steepest ascent. 

\begin{figure}[h]
\includegraphics[scale=0.40]{hw2-data/Bayes_classifier/newton_method_learning_objective.png}
\centering
\caption{plot the training function per each iteration for 10 runs for Newton's method}
\label{fig:newton_method_learning_objective}
\end{figure}

\pagebreak

\justify (e) the confusion matrix and the accuracy for Newton's method are shown below

\begin{center}
\begin{tabular}{ |c|c|c| } 
 \hline
  & Predicted 0  & Predicted 1 \\ 
 \hline
 Actual 0 & TN = 2648 & FP = 139 \\ 
  \hline
Actual 1  & FN = 219 &  TP = 1594 \\ 
 \hline
\end{tabular}
\end{center}
where TN is true negative, FP is false positive, FN is false negative, and TP is true positive. The overall accuracy is \textbf{$92.21\%$}

\pagebreak

\section* {Problem 3}

\justify (a) the table below shows the RMSE associated with every combination of b and $\sigma^{2}$. 

\begin{center}
\begin{tabular}{lrrrrrrrrrr}
\toprule
\;\;\;  $\sigma^2$ &   0.1 &   0.2 &   0.3 &   0.4 &   0.5 &   0.6 &   0.7 &   0.8 &   0.9 &   1.0 \\
b  &       &       &       &       &       &       &       &       &       &       \\
\midrule
5  &  1.97 &  1.93 &  1.92 &  1.92 &  1.92 &  1.93 &  1.93 &  1.94 &  1.95 &  1.95 \\
7  &  1.92 &  1.90 &  1.91 &  1.92 &  1.92 &  1.93 &  1.94 &  1.95 &  1.96 &  1.97 \\
9  &  1.90 &  1.90 &  1.92 &  1.93 &  1.95 &  1.96 &  1.97 &  1.98 &  1.98 &  1.99 \\
11 &  \textbf{1.89} &  1.91 &  1.94 &  1.96 &  1.97 &  1.99 &  2.00 &  2.01 &  2.01 &  2.02 \\
13 &  1.90 &  1.94 &  1.96 &  1.99 &  2.00 &  2.01 &  2.02 &  2.03 &  2.04 &  2.05 \\
15 &  1.91 &  1.96 &  1.99 &  2.01 &  2.03 &  2.04 &  2.05 &  2.06 &  2.07 &  2.07 \\
\bottomrule
\label{table:rmse_table}
\end{tabular}
\end{center}

\justify (b) Based on the cross validation results, the combination that generated the lowest RMSE in the test set is when $b=11$ and $\sigma^2=0.1$. Below is the metrics generated from the first homework using polynomial regression
\begin{itemize}
	\item 1st order polynomial regression: $\lambda = 0, RMSE_{min} \approx 2.634$
	\item 2nd order polynomial regression: $\lambda = 51, RMSE_{min} \approx 2.13$
	\item 3rd order polynomial regression: $\lambda = 52, RMSE_{min} \approx 2.10$
\end{itemize} 

\justify It seems that Gaussian Process had a better performance in terms of RMSE than the other polynomial regression models in the test set. A potential drawback of using the Gaussian Process (GP) compared to Homework 1 (ridge regression (RR) ) could be the fact that we integrate out the parameter W in order to use Kernel functions, as a consequence,  we couldn't look at each dimension of W to understand how features interact with the response variable (unlike homework 1 where we looked at how W changed with different $\lambda$ values). Although GP did improve RMSE, it was done at a cost of explainability. 

\pagebreak
\justify (c) Plot the predicted mean and the original data points from the training data
\begin{figure}[h]
\includegraphics[scale=0.40]{hw2-data/Gaussian_process/car_weight_rmse.png}
\centering
\caption{Predicted mean and the original data points}
\label{fig:car_weight_rmse}
\end{figure}

\end{document}